import numpy as np
import torch
from .ModelBase import IModel
# from diffusers.models.autoencoder_kl import AutoencoderKLOutput, DecoderOutput
from diffusers.models.autoencoders.autoencoder_kl import AutoencoderKLOutput, DecoderOutput
from attrs import define, field

@define(kw_only=True)
class VaeEncodeOutput:
    ''' parameters for a diagonal gaussian distribution
    '''
    mean : torch.Tensor = field(converter=lambda x : torch.tensor(x))
    std : torch.Tensor = field(converter=lambda x : torch.tensor(x))

class IDiffusionVae(IModel):
    def __init__(self, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)
        
    def encode(self, x: torch.Tensor, input_data_layout : str = 'nchw') -> VaeEncodeOutput:
        ''' encode image to latent space.
        
        parameters
        ---------------
        x : torch.Tensor
            image tensor in batch format, can be uint8 or float16/32. For float image, 
            the pixel value range is within [0,1].
        input_data_layout : str
            the layout of input data, can be 'nchw' or 'nhwc'
            
        return
        ----------
        VaeEncodeOutput
            the encode output, in nchw layout
        '''
        
        raise NotImplementedError()
    
    def decode(self, z: torch.Tensor, 
               output_dtype : torch.dtype | np.dtype = None, 
               output_data_layout : str = 'nchw') -> torch.Tensor:
        ''' decode latent to image.
        
        parameters
        ---------------
        z : torch.Tensor
            the latent tensor, in nchw layout
        output_dtype : torch.dtype | np.dtype
            the output data type, if None, will use the model dtype
        output_data_layout : str
            the output data layout, can be 'nchw' or 'nhwc'
            
        return
        ----------
        torch.Tensor
            the decoded image, in nchw layout
        '''
        raise NotImplementedError()
    
    @property
    def latent_scaling_factor(self) -> float:
        ''' when training stable diffusion, the vae latent is scaled. Specifically, 
        latent_of_diffusion = latent_of_vae * latent_scaling_factor.
        So, when using the vae to encode image, the latent should be scaled by this factor,
        when using the vae to decode latent, the latent should be divided by this factor.
        
        Note that this only applies to latent generated by stable diffusion. For those generated by vae encode/decode,
        you do not need to scale it.
        '''
        raise NotImplementedError()
    