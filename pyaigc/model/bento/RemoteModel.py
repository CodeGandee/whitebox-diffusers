import numpy as np
import torch
import bentoml as bml
import einops
from attrs import define, field
import attrs.validators as av
import dataclasses as dc
import aigc3to2.bento_service.message.msg as pkgmsg
from typing import Literal
from typing_extensions import Self

from diffusers.models.controlnet import ControlNetOutput

from pyaigc.model.DiffusionUnetBase import IDiffusionUnet
from pyaigc.model.DiffusionVaeBase import IDiffusionVae, VaeEncodeOutput
from pyaigc.model.DiffusionTextModelBase import IDiffusionTextModel
from pyaigc.model.ControlnetBase import IControlnet
import pyaigc.GlobalConfig as C

from igpy.common.shortfunc import to_4d_tensor
# import pyaigc.TextHandling as th

import igpy.common.shortfunc as sf

@define(kw_only=True)
class ClientModelBase:
    ''' a thin wrapper around bentoml client
    '''
    m_model_key : str = field(alias='model_key')
    m_client : bml.client.Client = field(default=None, alias='client')
    m_info : dict = field(factory=dict, init=False)
        
    @property
    def model_key(self) -> str:
        return self.m_model_key
        
    @property
    def client(self) -> bml.client.Client:
        return self.m_client
    
    @property
    def config(self) -> dict | None:
        ''' model config
        '''
        return self.m_info.get(pkgmsg.ModelQueryKeys.ModelConfig, None)
    
    @property
    def info(self) -> dict | None:
        return self.m_info
    
    @classmethod
    def from_url(cls, model_key : str, url : str) -> Self:
        out = cls(model_key=model_key)
        out.connect(url)
        out.pull_model_info()
        return out
    
    @classmethod
    def from_client(cls, model_key : str, client : bml.client.Client) -> Self:
        out = cls(model_key=model_key, client=client)
        out.pull_model_info()
        return out
    
    def pull_model_info(self):
        ''' get model info from server and cache it. By default, we query everything, and cache what is not None.
        '''
        msg = pkgmsg.ModelQueryInputMessage(
            modelKey=self.m_model_key,
            query_keys=pkgmsg.ModelQueryKeys.get_all_keys()
        )
        res = self.m_client.call('get_model_info_plain', msg.model_dump())
        query_results = pkgmsg.ModelQueryOutputMessage(**res).query_results
        self.m_info = {k:v for k,v in query_results.items() if v is not None}
        
    def connect(self, url : str):
        self.m_client = bml.client.Client.from_url(server_url=url)
        msg = pkgmsg.ModelQueryInputMessage(modelKey=self.m_model_key, with_model_config=True)
        
    def close(self):
        self.m_client.close()

@define(kw_only=True)
class DiffusionUnet(ClientModelBase, IDiffusionUnet):
    @property
    def scheduler_config(self) -> dict[str, object]:
        return self.info.get(pkgmsg.ModelQueryKeys.SchedulerConfig, None)
    
    @torch.no_grad()
    def forward(
        self,
        sample: torch.Tensor,
        timestep: torch.Tensor | float | int,
        prompt_embedding: torch.Tensor,
        cross_attention_kwargs: dict[str, any] = None,
        down_block_additional_residuals: tuple[torch.Tensor] = None,
        mid_block_additional_residual: torch.Tensor = None
    ) -> torch.Tensor:
        
        msg = pkgmsg.UnetInputMessage(
            modelKey=self.model_key, 
            sample=sample,
            prompt_embedding=prompt_embedding,
            timestep=timestep
        )
        msg.cross_attention_kwargs = cross_attention_kwargs
        if down_block_additional_residuals is not None:
            msg.down_block_additional_residuals = list(down_block_additional_residuals)
        msg.mid_block_additional_residual = mid_block_additional_residual
        
        res : dict = self.client.call('unet_forward_bin', pkgmsg.UniversalMessage.from_docarray_message(msg).model_dump())
        out : pkgmsg.UnetOutputMessage = pkgmsg.UnetOutputMessage.from_universal_message(pkgmsg.UniversalMessage(**res))
        return torch.tensor(out.output)
        
@define(kw_only=True)
class DiffusionVae(ClientModelBase, IDiffusionVae):    
    @property
    def latent_scaling_factor(self) -> float:
        ''' when training stable diffusion, the vae latent is scaled. Specifically, 
        latent_of_diffusion = latent_of_vae * latent_scaling_factor.
        So, when using the vae to encode image, the latent should be scaled by this factor,
        when using the vae to decode latent, the latent should be divided by this factor.
        
        Note that this only applies to latent generated by stable diffusion. For those generated by vae encode/decode,
        you do not need to scale it.
        '''
        return self.config.get('scaling_factor', 1.0)
    
    @torch.no_grad()
    def encode(self, x: torch.Tensor, input_data_layout : str = 'nchw') -> VaeEncodeOutput:
        from diffusers.models.vae import DiagonalGaussianDistribution
        
        msg = pkgmsg.VaeEncodeInputMessage(
            modelKey=self.model_key,
            image=x, 
            image_data_layout=input_data_layout, 
            with_logvar=True)
        res : dict = self.client.call('vae_encode_bin', pkgmsg.UniversalMessage.from_docarray_message(msg).model_dump())
        out : pkgmsg.VaeEncodeOutputMessage = pkgmsg.VaeEncodeOutputMessage.from_universal_message(pkgmsg.UniversalMessage(**res))
        return VaeEncodeOutput(mean=out.mean, std=np.exp(0.5*out.logvar))
    
    @torch.no_grad()
    def decode(self, z: torch.Tensor, 
               output_dtype : torch.dtype | np.dtype = None, 
               output_data_layout : str = 'nchw') -> torch.Tensor:

        if output_dtype is None:
            output_dtype = z.dtype
            
        msg = pkgmsg.VaeDecodeInputMessage(
            modelKey=self.model_key,
            latent=z, output_data_layout=output_data_layout, 
            output_dtype=output_dtype)
        res = self.client.call('vae_decode_bin', pkgmsg.UniversalMessage.from_docarray_message(msg).model_dump())
        out : pkgmsg.VaeDecodeOutputMessage = pkgmsg.VaeDecodeOutputMessage.from_universal_message(pkgmsg.UniversalMessage(**res))
        return torch.tensor(out.output)

@define(kw_only=True)
class DiffusionTextModel(ClientModelBase, IDiffusionTextModel):
    @property
    def config(self) -> dict[str, object]:
        ''' config of encoder and tokenizer
        '''
        return {
            'encoder': super().config,
            'tokenizer':None
        }
    
    @torch.no_grad()
    def encode(self, text:str) -> torch.Tensor:
        ''' encode text to tensor
        '''
        msg = pkgmsg.TextEncodeInputMessage(modelKey=self.model_key, text=text)
        res = self.client.call('encode_text_bin', pkgmsg.UniversalMessage.from_docarray_message(msg).model_dump())
        out : pkgmsg.TextEncodeOutputMessage = pkgmsg.TextEncodeOutputMessage.from_universal_message(pkgmsg.UniversalMessage(**res))
        return torch.tensor(out.output)
    
@define(kw_only=True)
class DiffusionControlnet(ClientModelBase, IControlnet):
    @torch.no_grad()
    def forward(
        self,
        sample: torch.FloatTensor,
        timestep: torch.Tensor | float | int,
        prompt_embedding: torch.Tensor,
        condition: torch.Tensor,
        condition_data_layout : Literal['nchw','nhwc'],
        conditioning_scale: float = 1.0,
        attention_mask: torch.Tensor = None,        
        cross_attention_kwargs: dict[str, object] = None,
    ) -> ControlNetOutput:
        
        msg = pkgmsg.ControlnetInputMessage(
            modelKey=self.model_key,
            sample=sample,
            timestep=timestep,
            prompt_embedding=prompt_embedding,
            condition=condition,
            condition_layout=condition_data_layout,
            weight=conditioning_scale,
            cross_attention_kwargs=cross_attention_kwargs,
            attention_mask=attention_mask
        )
        
        res = self.client.call('controlnet_forward_bin', pkgmsg.UniversalMessage.from_docarray_message(msg).model_dump())
        out : pkgmsg.ControlnetOutputMessage = pkgmsg.ControlnetOutputMessage.from_universal_message(pkgmsg.UniversalMessage(**res))
        _out = ControlNetOutput(out.down_block_res_samples, out.mid_block_res_sample)
        return _out